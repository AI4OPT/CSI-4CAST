config: !!python/object:src.cp.config.config.ExperimentConfig
  accelerator: auto
  data: !!python/object:src.cp.config.config.DataConfig
    batch_size: 16
    dir_dataset: z_artifacts/data
    is_U2D: false
    is_separate_antennas: true
    num_workers: 1
    shuffle: true
    train_ratio: 0.9
  deterministic: false
  devices: 1
  experiment_name: TDD_RNN
  loss: &id001 !!python/object:src.cp.config.config.LossConfig
    name: NMSE
    params: {}
  model: &id002 !!python/object:src.cp.config.config.ModelConfig
    checkpoint_path: null
    is_separate_antennas: true
    name: RNN_TDD
    params:
      dim_data: 600
      pred_len: 4
      rnn_hidden_dim: 1200
      rnn_num_layers: 4
  model_name: RNN
  optimizer: &id003 !!python/object:src.cp.config.config.OptimizerConfig
    name: Adam
    params:
      betas:
      - 0.9
      - 0.999
      eps: 1.0e-08
      lr: 0.0005
      weight_decay: 0.0001
  output_dir: z_artifacts/outputs/TDD/RNN
  precision: '32'
  prefix: TDD
  scheduler: &id004 !!python/object:src.cp.config.config.SchedulerConfig
    name: ReduceLROnPlateau
    params:
      cooldown: 5
      factor: 0.1
      min_lr: 1.0e-06
      mode: min
      patience: 10
      threshold: 0.0001
  seed: 42
  training: !!python/object:src.cp.config.config.TrainingConfig
    accumulate_grad_batches: 1
    check_val_every_n_epoch: 1
    early_stopping: true
    early_stopping_min_delta: 0.0001
    early_stopping_mode: min
    early_stopping_patience: 20
    enable_model_summary: true
    enable_progress_bar: true
    gradient_clip_val: null
    log_every_n_steps: 50
    monitor_metric: val_loss
    monitor_mode: min
    num_epochs: 500
    save_top_k: 1
loss_config: *id001
model: *id002
optimizer_config: *id003
scheduler_config: *id004
