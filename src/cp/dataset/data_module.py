"""PyTorch Lightning Data Module for CSI Prediction.

This module implements a comprehensive data loading and preprocessing pipeline for CSI prediction
models using PyTorch Lightning. It handles subset-stratified data splitting, normalization,
noise addition, and efficient data loading for training and validation.

Key Features:
- Subset-stratified splitting: Ensures all 27 parameter combinations are represented in both
  training and validation sets while preventing data leakage
- Automatic data normalization using pre-computed statistics
- Realistic noise addition during training and validation
- Flexible antenna handling (separate vs. gathered antennas)
- Memory-efficient data loading with proper collation functions

The data module loads CSI data from multiple scenarios defined by:
- Channel models: A, C, D (3 models)
- Delay spreads: 30ns, 100ns, 300ns (3 values)
- Mobility speeds: 1, 5, 15 m/s (3 values)
Total: 3 √ó 3 √ó 3 = 27 parameter combinations

Usage:
    from src.cp.dataset.data_module import TrainValDataModule

    data_module = TrainValDataModule(data_config)
    data_module.setup()

    train_loader = data_module.train_dataloader()
    val_loader = data_module.val_dataloader()
"""

import logging
import random
from pathlib import Path

import lightning.pytorch as pl
import torch
from torch.utils.data import DataLoader

from src.cp.config.config import DataConfig
from src.noise.noise import gen_vanilla_noise_snr
from src.utils.data_utils import (
    LIST_CHANNEL_MODEL,
    LIST_DELAY_SPREAD,
    LIST_MIN_SPEED_TRAIN,
    SNR_RANGE_GAUSSIAN_NOISE_TRAIN,
    CSIDataset,
    _load_data,
    collect_fn_gather_antennas,
    collect_fn_separate_antennas,
)
from src.utils.norm_utils import normalize_input


logger = logging.getLogger(__name__)


class TrainValDataModule(pl.LightningDataModule):
    """PyTorch Lightning Data Module for CSI Training and Validation.

    This data module implements subset-stratified data splitting to ensure that all 27
    parameter combinations (channel model √ó delay spread √ó mobility speed) are represented
    proportionally in both training and validation sets. This approach prevents data leakage
    while ensuring representative validation performance.

    The module handles:
    - Data loading from multiple CSI scenarios
    - Subset-stratified train/validation splitting
    - Data normalization using pre-computed statistics
    - Noise addition for realistic training conditions
    - Flexible data loading with configurable batch sizes and collation functions

    Args:
        data_config (DataConfig): Configuration object containing data loading parameters

    """

    def __init__(self, data_config: DataConfig):
        """Initialize the data module with configuration.

        Args:
            data_config (DataConfig): Data configuration containing:
                - dir_dataset: Path to dataset directory
                - batch_size: Batch size for data loaders
                - train_ratio: Ratio of data to use for training (0.0-1.0)
                - is_U2D: Whether to use U2D (FDD) or TDD scenario
                - is_separate_antennas: Whether to process antennas separately
                - shuffle: Whether to shuffle training data

        """
        super().__init__()
        self.data_cfg = data_config

    def setup(self, stage=None):
        """Setup datasets for training and validation with subset-stratified splitting.

        This method is called by PyTorch Lightning before training begins. It loads
        all CSI data, performs subset-stratified splitting, applies normalization,
        adds noise, and creates the final datasets.

        Args:
            stage (str, optional): Training stage ('fit', 'validate', 'test', 'predict')

        """
        # Use subset-stratified splitting to ensure all 27 subsets are represented in both train and val
        self._setup_subset_stratified_split()

    def _setup_subset_stratified_split(self):
        """Load data with subset-stratified splitting.

        This method implements a sophisticated data splitting strategy where each of the 27
        parameter combinations (subsets) contributes proportionally to both training and
        validation sets. This ensures:

        1. Representative validation: All scenarios are represented in validation
        2. No data leakage: Training and validation come from the same distributions
        3. Balanced evaluation: Each scenario contributes equally to validation metrics

        The splitting process:
        1. Load each of the 27 subsets individually
        2. Split each subset according to train_ratio using consistent random seeding
        3. Aggregate all training splits and all validation splits
        4. Apply normalization and noise to the aggregated datasets
        """
        H_hist_train_list = []
        H_pred_train_list = []
        H_hist_val_list = []
        H_pred_val_list = []

        total_subsets = 0
        total_train_samples = 0
        total_val_samples = 0

        logger.info(f"üîÑ Loading data with subset-stratified splitting (train_ratio={self.data_cfg.train_ratio})")

        # Process each subset individually
        for cm in LIST_CHANNEL_MODEL:
            for ds in LIST_DELAY_SPREAD:
                for ms in LIST_MIN_SPEED_TRAIN:
                    # Load this specific subset
                    H_hist_subset = _load_data(
                        dir_data=Path(self.data_cfg.dir_dataset),
                        cm=cm,
                        ds=ds,
                        ms=ms,
                        is_train=True,
                        is_gen=False,
                        is_hist=True,
                        is_U2D=self.data_cfg.is_U2D,
                    )

                    H_pred_subset = _load_data(
                        dir_data=Path(self.data_cfg.dir_dataset),
                        cm=cm,
                        ds=ds,
                        ms=ms,
                        is_train=True,
                        is_gen=False,
                        is_hist=False,
                        is_U2D=self.data_cfg.is_U2D,
                    )

                    # Split this subset according to train_ratio
                    subset_size = len(H_hist_subset)
                    train_size = int(subset_size * self.data_cfg.train_ratio)

                    # Use consistent indexing for reproducible splits
                    indices = torch.randperm(subset_size, generator=torch.Generator().manual_seed(42))
                    train_indices = indices[:train_size]
                    val_indices = indices[train_size:]

                    # Split the subset
                    H_hist_train_subset = H_hist_subset[train_indices]
                    H_pred_train_subset = H_pred_subset[train_indices]
                    H_hist_val_subset = H_hist_subset[val_indices]
                    H_pred_val_subset = H_pred_subset[val_indices]

                    # Add to respective lists
                    H_hist_train_list.append(H_hist_train_subset)
                    H_pred_train_list.append(H_pred_train_subset)
                    H_hist_val_list.append(H_hist_val_subset)
                    H_pred_val_list.append(H_pred_val_subset)

                    # Track statistics
                    total_subsets += 1
                    total_train_samples += len(H_hist_train_subset)
                    total_val_samples += len(H_hist_val_subset)

                    logger.info(
                        f"üìä Subset CM={cm}, DS={round(ds * 1e9)}ns, MS={ms}: "
                        f"{subset_size} total ‚Üí {len(H_hist_train_subset)} train, {len(H_hist_val_subset)} val"
                    )

        # Concatenate all training data
        H_hist_train = torch.cat(H_hist_train_list, dim=0)
        H_pred_train = torch.cat(H_pred_train_list, dim=0)

        # Concatenate all validation data
        H_hist_val = torch.cat(H_hist_val_list, dim=0)
        H_pred_val = torch.cat(H_pred_val_list, dim=0)

        logger.info("‚úÖ Stratified split completed:")
        logger.info(f"   üìà {total_subsets} subsets processed")
        logger.info(f"   üèãÔ∏è Training: {total_train_samples} samples from ALL subsets")
        logger.info(f"   üîç Validation: {total_val_samples} samples from ALL subsets")
        logger.info(f"   üìè Data shapes: hist={H_hist_train[0].shape}, pred={H_pred_train[0].shape}")

        # Normalize the input data
        H_hist_train, H_pred_train = normalize_input(H_hist_train, H_pred_train, is_U2D=self.data_cfg.is_U2D)
        H_hist_val, H_pred_val = normalize_input(H_hist_val, H_pred_val, is_U2D=self.data_cfg.is_U2D)

        assert H_pred_train is not None, "H_pred_train should not be None"
        assert H_pred_val is not None, "H_pred_val should not be None"

        # Add noise to both training and validation data (realistic scenario)
        logger.info("üîä Adding noise to training data...")
        for i in range(len(H_hist_train)):
            snr_val = random.uniform(*SNR_RANGE_GAUSSIAN_NOISE_TRAIN)
            H_hist_train[i] = H_hist_train[i] + gen_vanilla_noise_snr(H_hist_train[i], SNR=snr_val)

        logger.info("üîä Adding noise to validation data...")
        for i in range(len(H_hist_val)):
            snr_val = random.uniform(*SNR_RANGE_GAUSSIAN_NOISE_TRAIN)
            H_hist_val[i] = H_hist_val[i] + gen_vanilla_noise_snr(H_hist_val[i], SNR=snr_val)

        # Create datasets
        self.train_dataset = CSIDataset(H_hist_train, H_pred_train)
        self.val_dataset = CSIDataset(H_hist_val, H_pred_val)

        logger.info(f"üéØ Final datasets: Train={len(self.train_dataset)}, Val={len(self.val_dataset)}")

    def train_dataloader(self):
        """Create training data loader.

        Returns:
            DataLoader: PyTorch DataLoader for training data with appropriate
                       collation function based on antenna configuration

        """
        # Select collation function based on antenna processing mode
        if self.data_cfg.is_separate_antennas:
            collate_fn = collect_fn_separate_antennas  # Process each antenna separately
        else:
            collate_fn = collect_fn_gather_antennas  # Process all antennas together

        return DataLoader(
            self.train_dataset,
            batch_size=self.data_cfg.batch_size,
            shuffle=self.data_cfg.shuffle,
            num_workers=0,  # Set to 0 to avoid multiprocessing issues on some systems
            collate_fn=collate_fn,
        )

    def val_dataloader(self):
        """Create validation data loader.

        Returns:
            DataLoader: PyTorch DataLoader for validation data with appropriate
                       collation function based on antenna configuration

        """
        # Select collation function based on antenna processing mode
        if self.data_cfg.is_separate_antennas:
            collate_fn = collect_fn_separate_antennas  # Process each antenna separately
        else:
            collate_fn = collect_fn_gather_antennas  # Process all antennas together

        return DataLoader(
            self.val_dataset,
            batch_size=self.data_cfg.batch_size,
            shuffle=False,  # No shuffling for validation
            num_workers=0,  # Set to 0 to avoid multiprocessing issues on some systems
            collate_fn=collate_fn,
        )

    def get_data_shapes(self):
        """Get the shapes of data tensors from the training dataset.

        This method is useful for model initialization and debugging to understand
        the expected input and output tensor dimensions.

        Returns:
            tuple: (history_shape, prediction_shape) where:
                - history_shape: Shape of historical CSI input tensors
                - prediction_shape: Shape of prediction target tensors
                - Returns (None, None) if dataset is not yet set up

        """
        if not hasattr(self, "train_dataset") or self.train_dataset is None or len(self.train_dataset) == 0:
            return None, None

        # Get a batch from the training loader to determine shapes
        loader = self.train_dataloader()
        hist_batch, pred_batch = next(iter(loader))

        return hist_batch.shape, pred_batch.shape


if __name__ == "__main__":
    # Example usage
    data_config = DataConfig()
    data_module = TrainValDataModule(data_config)
    data_module.setup()

    print("‚úÖ Subset-stratified data module created successfully!")
    print(f"   üèãÔ∏è Training samples: {len(data_module.train_dataset)}")
    print(f"   üîç Validation samples: {len(data_module.val_dataset)}")
    print("   üìä All 27 subsets represented in both train and validation")
